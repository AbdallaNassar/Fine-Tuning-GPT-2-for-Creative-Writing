{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('text_data.csv')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['cleaned_text'] = df['text_column'].apply(clean_text)\n",
        "print(df[['text_column', 'cleaned_text']].head())\n",
        "df.to_csv('cleaned_text_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports & Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intallation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Library         | Description                                                                                                           |\n",
        "|-----------------|-----------------------------------------------------------------------------------------------------------------------|\n",
        "| transformers    | A library that provides state-of-the-art pretrained models for various NLP tasks.                                     |\n",
        "| datasets        | A library that simplifies the process of accessing and working with a wide range of machine learning datasets.        |\n",
        "| mlflow          | A platform for managing the end-to-end machine learning lifecycle, from experimentation to deployment.                |\n",
        "| torch (PyTorch) | A powerful deep learning framework used for building, training, and deploying neural networks.                        |\n",
        "| pyngrok         | A tool that allows local servers (like Gradio apps) to be exposed to the internet for easy testing and sharing.       |\n",
        "| gradio          | A user-friendly library for creating interactive UIs for machine learning models, enabling easy sharing and testing.  |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ME_lHf_jtpEM",
        "outputId": "426749e7-25ff-4f64-a853-61275032ffcd"
      },
      "outputs": [],
      "source": [
        "        # You can use this cell if you don't need to run requirements.txt\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install mlflow\n",
        "# !pip install torch\n",
        "# !pip install pyngrok -q\n",
        "# !pip install gradio\n",
        "# !pip install accelerate>=0.26.0\n",
        "# !ngrok config add-authtoken 0000000000000000000000000000000000000000000000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e_rlESc7BbTJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules for subprocess management\n",
        "import subprocess\n",
        "# Import pyngrok for handling public access tunnels and configurations\n",
        "from pyngrok import ngrok, conf\n",
        "# For securely handling password inputs\n",
        "import getpass\n",
        "# Importing os module to interact with the operating system\n",
        "import os\n",
        "# Importing MLflow to track machine learning experiments with PyTorch models\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "# Import transformers' pre-trained GPT-2 model and tokenizer, as well as Trainer utilities\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "# For loading and handling datasets\n",
        "from datasets import load_dataset, DatasetDict\n",
        "# Import PyTorch, a machine learning framework\n",
        "import torch\n",
        "# Import pre-trained models and tokenizers for causal language modeling tasks\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# Importing Gradio, a framework to create web interfaces for machine learning models\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initializing MLflow Tracking with a SQLite Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDCn1K8XBi_T",
        "outputId": "87140ff9-11de-45f3-ba7e-50dfbb0fb307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['mlflow', 'ui', '--backend-store-uri', 'sqli...>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the URI for MLflow to use a SQLite database as the backend store for tracking experiments.\n",
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "\n",
        "# Start the MLflow tracking UI in a new process, using the specified SQLite database as the backend store.\n",
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Establishing MLflow Tracking Configuration for Experiment Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-LKOiRBmOU",
        "outputId": "8220f325-69a6-4d19-895d-9a8430c6611a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:d:/MO/Ai Projects/creative writing/mlruns/1', creation_time=1728799754107, experiment_id='1', last_update_time=1728799754107, lifecycle_stage='active', name='duration-prediction-experiment', tags={}>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the MLflow tracking URI to specify where the tracking data will be stored.\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "# Set the name of the experiment to track runs under a specific experiment name in MLflow.\n",
        "mlflow.set_experiment(\"duration-prediction-experiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuring ngrok with Authentication Token for Secure Tunneling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V662CxOaB5je",
        "outputId": "b64b5726-cd08-441a-bf2d-43d03b944965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
            " * ngrok tunnel \"https://b8d8-154-239-194-127.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
          ]
        }
      ],
      "source": [
        "# Prompt the user to enter their ngrok authentication token\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "\n",
        "# Get the authentication token securely (input will not be shown on the console)\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "# Set the local port number that the ngrok tunnel will forward to\n",
        "port = 5000\n",
        "\n",
        "# Establish an ngrok tunnel to the specified local port and retrieve the public URL\n",
        "public_url = ngrok.connect(port).public_url\n",
        "\n",
        "# Print the public URL provided by ngrok, which forwards to the local server\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating MLflow Directory and Starting a New Experiment Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ActiveRun: >"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a directory named \"mlruns\" to store MLflow tracking data.\n",
        "# The exist_ok=True parameter means that no error will be raised if the directory already exists.\n",
        "os.makedirs(\"mlruns\", exist_ok=True)\n",
        "\n",
        "# End any active MLflow run to ensure that there are no overlapping runs.\n",
        "# This is useful to clean up before starting a new run.\n",
        "mlflow.end_run()\n",
        "\n",
        "# Start a new MLflow run to track metrics, parameters, and models associated with this particular experiment.\n",
        "mlflow.start_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Dataset and Configuring GPT-2 for Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the path to the cleaned dataset CSV file\n",
        "data_files = 'cleaned_creative_writing_dataset.csv'\n",
        "\n",
        "# Load the dataset from the specified CSV file\n",
        "# The 'csv' argument indicates the file format, and the 'data_files' argument specifies the path to the file\n",
        "dataset = load_dataset('csv', data_files=data_files)\n",
        "\n",
        "# Remove the 'text' column from the dataset\n",
        "# This is done to avoid any potential conflicts or redundant information\n",
        "dataset = dataset['train'].remove_columns(['text'])\n",
        "\n",
        "# Rename the 'cleaned_text' column to 'text' for consistency\n",
        "# This makes it easier to refer to the main text column in subsequent processing\n",
        "dataset = dataset.rename_column('cleaned_text', 'text')\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "# The tokenizer is responsible for converting text into token IDs that the model can understand\n",
        "# it uses BPETokenizer for subword tokenization and Byte-Pair Encoding (BPE) algorithm, it has 117M parameters\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "# The 'GPT2LMHeadModel' is the model architecture that can generate text\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the tokenizer's padding token to the end-of-sequence token\n",
        "# This is important for ensuring that input sequences have consistent lengths during training or inference\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Token IDs    : [40, 1842, 22875, 9078]\n",
            "Tokenized Words  : ['I', 'Ä love', 'Ä Dot', 'py']\n"
          ]
        }
      ],
      "source": [
        "# Tokenization example on a sample sentence\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "sentence = \"I love Dotpy\"\n",
        "tokens = tokenizer.encode(sentence)\n",
        "tokenized_words = tokenizer.convert_ids_to_tokens(tokens)\n",
        "print(\"    Token IDs    :\", tokens)\n",
        "print(\"Tokenized Words  :\", tokenized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs      :\n",
            "tensor([   40,  1842, 22875, 20519, 50256, 50256, 50256])\n",
            "tensor([   35,   313, 20519,   318,  4998,   290,  3608])\n",
            "Tokenized Words:\n",
            "['I', 'Ä love', 'Ä Dot', 'Py', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
            "['D', 'ot', 'Py', 'Ä is', 'Ä amazing', 'Ä and', 'Ä cool']\n"
          ]
        }
      ],
      "source": [
        "sequences = [\"I love DotPy\", \"DotPy is amazing and cool\"]\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokens = tokenizer(sequences, padding=True, return_tensors='pt')\n",
        "tokenized_output = [tokenizer.convert_ids_to_tokens(token_ids) for token_ids in tokens['input_ids']]\n",
        "print(\"Token IDs      :\", *tokens['input_ids'], sep='\\n')\n",
        "print(\"Tokenized Words:\", *tokenized_output,sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preprocessing: Tokenization and Train-Test Split for GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a tokenization function to process the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the 'text' field from the dataset examples using the pre-loaded tokenizer\n",
        "    # padding='max_length' ensures that all sequences are padded to the maximum length\n",
        "    # truncation=True cuts off sequences that exceed the max_length\n",
        "    # max_length=32 sets a fixed length of 32 tokens for each input\n",
        "    input_ids = tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',  # Pads to 32 tokens per sequence\n",
        "        truncation=True,       # Truncates sequences longer than 32 tokens\n",
        "        max_length=32          # Sets the maximum token length to 32\n",
        "    )\n",
        "\n",
        "    # Copy the 'input_ids' into a new field 'labels' to use as the target for training\n",
        "    # This is often done in language models to predict the next word in a sequence\n",
        "    input_ids['labels'] = input_ids['input_ids'].copy()\n",
        "    \n",
        "    # Return the tokenized input dictionary, including both 'input_ids' and 'labels'\n",
        "    return input_ids\n",
        "\n",
        "# Apply the tokenization function to the entire dataset\n",
        "# The map() method applies the function to each example in the dataset, with batched=True\n",
        "# meaning that multiple examples are passed in a single batch for faster processing\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split the tokenized dataset into a training set and a validation set\n",
        "# train_test_split(test_size=0.2) splits 80% of the data for training and 20% for validation\n",
        "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Organize the train and validation datasets into a DatasetDict for easy reference\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': train_test_split['train'],        # Training dataset (80% of the data)\n",
        "    'validation': train_test_split['test']     # Validation dataset (20% of the data)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training GPT-2 with Custom Hyperparameters and Logging with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\MO\\Depi\\envai\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "d:\\MO\\Depi\\envai\\Lib\\site-packages\\transformers\\training_args.py:1560: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
            "  warnings.warn(\n",
            "2024/10/19 16:02:37 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 89472e3da6cf45ba9d0ff405f8cd676f: Failed to log run data: Exception: Changing param values is not allowed. Params were already logged='[{'key': 'max_length', 'old_value': '32', 'new_value': '20'}]' for run ID='89472e3da6cf45ba9d0ff405f8cd676f'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f77d7d2f3dd4e54b7d994deb5190fdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1143 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.7737, 'grad_norm': 44.217384338378906, 'learning_rate': 1.1251093613298338e-05, 'epoch': 0.44}\n",
            "{'loss': 5.7421, 'grad_norm': 14.4274320602417, 'learning_rate': 2.502187226596676e-06, 'epoch': 0.87}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7588d27a1e78471bb791f57c00251062",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.446900367736816, 'eval_accuracy': 0.17755681818181818, 'eval_runtime': 28.9373, 'eval_samples_per_second': 9.883, 'eval_steps_per_second': 1.244, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1645.038, 'train_samples_per_second': 0.695, 'train_steps_per_second': 0.695, 'train_loss': 5.7256335694854545, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1143, training_loss=5.7256335694854545, metrics={'train_runtime': 1645.038, 'train_samples_per_second': 0.695, 'train_steps_per_second': 0.695, 'total_flos': 18666049536000.0, 'train_loss': 5.7256335694854545, 'epoch': 1.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function to compute evaluation metrics\n",
        "# This function will be called during the evaluation phase of the model\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred  # Extract the logits (model outputs) and true labels\n",
        "    predictions = logits.argmax(axis=-1)  # Get the predicted class by taking the argmax along the last axis\n",
        "    accuracy = (predictions == labels).mean()  # Compute the accuracy by comparing predictions with labels\n",
        "    return {'accuracy': accuracy}  # Return accuracy as a dictionary for logging\n",
        "\n",
        "# Define training hyperparameters\n",
        "learning_rate = 2e-5  # The learning rate for the optimizer\n",
        "per_device_train_batch_size = 1  # Batch size per device (1 sample per training step)\n",
        "num_train_epochs = 1  # Number of training epochs\n",
        "max_length = 32  # Maximum sequence length for inputs\n",
        "\n",
        "# Set up training arguments using Hugging Face's TrainingArguments class\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Directory where results (like checkpoints and logs) will be saved\n",
        "    evaluation_strategy='epoch',  # Evaluate the model at the end of each epoch\n",
        "    save_strategy='epoch',  # Save the model at the end of each epoch\n",
        "    learning_rate=learning_rate,  # Set the learning rate for training\n",
        "    per_device_train_batch_size=per_device_train_batch_size,  # Set the batch size per device\n",
        "    num_train_epochs=num_train_epochs,  # Define the number of training epochs\n",
        "    weight_decay=0.01,  # Weight decay to avoid overfitting (used in regularization)\n",
        "    load_best_model_at_end=True,  # Load the best model based on evaluation metrics after training ends\n",
        "    metric_for_best_model=\"accuracy\",  # The metric used to select the best model (accuracy in this case)\n",
        "    no_cuda=True,  # Force training on CPU, set to False if using GPU\n",
        ")\n",
        "\n",
        "# Instantiate a Trainer to manage the training loop\n",
        "trainer = Trainer(\n",
        "    model=model,  # The pre-trained GPT-2 model that you want to fine-tune\n",
        "    args=training_args,  # Training arguments defined above\n",
        "    train_dataset=tokenized_datasets['train'],  # The training dataset\n",
        "    eval_dataset=tokenized_datasets['validation'],  # The validation dataset for evaluation\n",
        "    compute_metrics=compute_metrics,  # The function to compute evaluation metrics (accuracy here)\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Early stopping callback to avoid overfitting\n",
        "    # The model stops training if it doesn't improve for 3 evaluation cycles (epochs in this case)\n",
        ")\n",
        "\n",
        "# Log hyperparameters using MLflow\n",
        "mlflow.log_param(\"data_files\", data_files)  # Log the dataset file used for training\n",
        "mlflow.log_param(\"learning_rate\", learning_rate)  # Log the learning rate used\n",
        "mlflow.log_param(\"per_device_train_batch_size\", per_device_train_batch_size)  # Log batch size\n",
        "mlflow.log_param(\"num_train_epochs\", num_train_epochs)  # Log number of epochs\n",
        "mlflow.log_param(\"max_length\", max_length)  # Log the max sequence length for tokenization\n",
        "mlflow.log_param(\"model_name\", \"gpt2\")  # Log the model name (GPT-2 in this case)\n",
        "\n",
        "# Start training the model using the Trainer instance\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation and Metrics Logging with MLflow for GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Completion of Training: Saving Model, Logging Metrics, and Ending MLflow Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/10/19 16:32:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48aa487a2589449ba19001792091c14a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training and saving completed.\n"
          ]
        }
      ],
      "source": [
        "# Save the fine-tuned model and tokenizer locally\n",
        "model.save_pretrained('./fine_tuned_gpt2')  # Save the fine-tuned GPT-2 model to the specified directory\n",
        "tokenizer.save_pretrained('./fine_tuned_gpt2')  # Save the tokenizer (required for text preprocessing) to the same directory\n",
        "\n",
        "# Log the model to MLflow using the PyTorch logging interface\n",
        "# This will store the model artifact in the MLflow tracking system for later use\n",
        "mlflow.pytorch.log_model(model, \"fine_tuned_gpt2\")\n",
        "\n",
        "# Evaluate the model using the trainer and store the evaluation metrics (e.g., loss, accuracy)\n",
        "eval_metrics = trainer.evaluate()\n",
        "\n",
        "# Extract the training loss from the trainer's state history if it exists\n",
        "# The state.log_history holds a record of logs during training\n",
        "if 'loss' in trainer.state.log_history[-1]:\n",
        "    train_loss = trainer.state.log_history[-1]['loss']  # Get the last logged training loss\n",
        "else:\n",
        "    train_loss = None  # If not found, set training loss to None\n",
        "\n",
        "# Log metrics to MLflow\n",
        "mlflow.log_metric(\"Training Loss\", train_loss if train_loss is not None else 0.0)  # Log training loss (set to 0.0 if unavailable)\n",
        "mlflow.log_metric(\"Validation Loss\", eval_metrics['eval_loss'])  # Log the validation loss from evaluation\n",
        "mlflow.log_metric(\"Accuracy\", eval_metrics.get('eval_accuracy', 0.0))  # Log the accuracy (default to 0.0 if not found)\n",
        "\n",
        "# End the MLflow run to ensure all logs and artifacts are finalized\n",
        "mlflow.end_run()\n",
        "\n",
        "# Print confirmation message to indicate that the model training and saving process is complete\n",
        "print(\"Model training and saving completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Text with GPT-2: Story Creation and Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a story about a girl's adventures in a magical forest where she finds strange creatures always found dreams so natural, her mother feasts far later upon story stories saying first son feasting children feasting strange trees would never come though memories go never come canard later memories could no need even children find children without strange encounters never found life no wonder creatures love children even living creatures known strange creatures found many animals even life find others time appear strange even dead man mike scipaul wrote first man half grown could be human later writing tales almost every account of could be like first man year aged born man aged sixteen oldest could still alive year without words narrator james raul quanich holds no love love love became would make life love one final time write new poem still unknown characters love grows would die even although death begins never felt anyone can can actually make anything love mike snobbish shuck far away sounds sense ever after words still found many stories never coming come back true story one reason why two canons one poem tell story man wishes never can meet old man he soon meets shad elof could exist many stories dream tell story shad elof appeared ill ill story would exist story first day begins three first year may continue since may years dreams first love love will still survive many long stories story first men became canondylan men finally true life takes short joy mike snobbishly first great known woman found mysterious female friends told many stories half old human thought true desire mike appeared far less far beyond dead without world ever man loved love memories forgotten often also mythically creatures love living mortal became may dream living life dead almost could hear sounds could also find old men even dreams fear may live\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model and tokenizer from the specified directory\n",
        "model_name = \"./fine_tuned_gpt2\"  # Path to the saved fine-tuned model directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer associated with the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)  # Load the model for causal language modeling\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# This is essential for inference, disabling dropout and other training-specific behaviors\n",
        "model.eval()\n",
        "\n",
        "# Define a function to generate stories based on a given prompt\n",
        "# max_length: maximum length of the generated story\n",
        "# temperature: controls randomness in the generation process (higher values = more random)\n",
        "# top_k: limits the sampling pool to the top-k most likely next words\n",
        "def generate_story(prompt, max_length=1000, temperature=1.5, top_k=100):\n",
        "    # Tokenize the input prompt and convert it into input IDs (tensor format)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')  # 'pt' indicates PyTorch tensors\n",
        "    \n",
        "    # Disable gradient computation during generation for efficiency\n",
        "    with torch.no_grad():\n",
        "        # Generate text from the input prompt\n",
        "        output = model.generate(\n",
        "            input_ids,  # The input prompt as tokenized IDs\n",
        "            max_length=max_length,  # Maximum length of the generated sequence\n",
        "            temperature=temperature,  # Controls diversity in the output\n",
        "            top_k=top_k,  # Limits sampling to the top-k most probable tokens\n",
        "            do_sample=True,  # Enables sampling for more varied text generation\n",
        "            num_return_sequences=1,  # Generate only one sequence\n",
        "            pad_token_id=tokenizer.eos_token_id  # Use EOS token for padding\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens back into human-readable text\n",
        "    generated_story = tokenizer.decode(output[0], skip_special_tokens=True)  # Skip special tokens in the output\n",
        "\n",
        "    # Start a new MLflow run to log parameters related to text generation\n",
        "    mlflow.start_run()\n",
        "    mlflow.log_param(\"generation_max_length\", max_length)  # Log the maximum length for generation\n",
        "    mlflow.log_param(\"temperature\", temperature)  # Log the temperature setting\n",
        "    mlflow.log_param(\"top_k\", top_k)  # Log the top-k value used for sampling\n",
        "    mlflow.log_param(\"prompt\", prompt)  # Log the prompt that was used for generation\n",
        "    mlflow.end_run()  # End the MLflow run to save the logged parameters\n",
        "\n",
        "    return generated_story  # Return the generated story\n",
        "\n",
        "# Define a prompt to initiate the story generation\n",
        "prompt = \"Write a story about a girl's adventures in a magical forest where she finds strange creatures\"\n",
        "# Generate the story based on the prompt and specified parameters\n",
        "generated_text = generate_story(prompt, max_length=1000)  # Adjust max_length as needed\n",
        "# Print the generated story\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradio-Powered Story Generation: Generate Tales with Fine-Tuned GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "kR_PruYuD9mg",
        "outputId": "d3662e68-8f53-4dbc-d704-e36dbd6c3d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* Running on public URL: https://6399ed0431c95b509e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6399ed0431c95b509e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Gradio for creating web interfaces\n",
        "import gradio as gr\n",
        "\n",
        "# Define a wrapper function for generating stories using the previously defined generate_story function\n",
        "def gradio_generate(prompt):\n",
        "    # Call the generate_story function with the provided prompt to get the generated text\n",
        "    generated_text = generate_story(prompt)  # This utilizes the fine-tuned model to generate the story\n",
        "    return generated_text  # Return the generated story for display in the Gradio interface\n",
        "\n",
        "# Create a Gradio interface\n",
        "gradio_interface = gr.Interface(\n",
        "    fn=gradio_generate,  # Function that will be called to generate text\n",
        "    inputs=\"text\",  # Input type is a text box for users to enter their prompts\n",
        "    outputs=\"text\",  # Output type is a text box for displaying the generated story\n",
        "    title=\"Story Hallucinator\",  # Title of the Gradio app displayed at the top\n",
        "    description=\"Enter a prompt to generate a story using the fine-tuned GPT-2 model.\",  # Description shown to users\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "# The share=True option allows the interface to be shared via a public link\n",
        "gradio_interface.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
